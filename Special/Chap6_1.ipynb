{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85YNYOS7yWyC"
      },
      "source": [
        "6.1 Cleaning Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Le1g7_3ZxlgY"
      },
      "outputs": [],
      "source": [
        "# Create text\n",
        "text_data = [\"   Interrobang. By Aishwarya Henriette\"\n",
        "             \"Parking And Going. By Karl Gautier\",\n",
        "             \" Today Is The night. By Jarek Prakash   \"]\n",
        "# Strip whitespaces\n",
        "strip_whitespace = [string.strip() for string in text_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzsq8aEPzXxM",
        "outputId": "65dae267-ed18-4f2f-9585-fefc28c1555a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Interrobang. By Aishwarya HenrietteParking And Going. By Karl Gautier',\n",
              " 'Today Is The night. By Jarek Prakash']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "strip_whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XywAyZpbzfOK"
      },
      "outputs": [],
      "source": [
        "strip_whitespace = [string.lstrip() for string in text_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tku71pyazjFg",
        "outputId": "310ef803-9171-4fcf-9c3a-17e89d345bf9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Interrobang. By Aishwarya HenrietteParking And Going. By Karl Gautier',\n",
              " 'Today Is The night. By Jarek Prakash   ']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "strip_whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X2sSx1aszmwE"
      },
      "outputs": [],
      "source": [
        "strip_whitespace = [string.rstrip() for string in text_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH67_2dszpsT",
        "outputId": "68dbd2d6-3a3f-4988-cbfa-0343efd00096"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['   Interrobang. By Aishwarya HenrietteParking And Going. By Karl Gautier',\n",
              " ' Today Is The night. By Jarek Prakash']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "strip_whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaX_RiFOzReA",
        "outputId": "1e149a10-711c-4559-97fd-56eb1f3ee044"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['   Interrobang By Aishwarya HenrietteParking And Going By Karl Gautier',\n",
              " ' Today Is The night By Jarek Prakash']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove periods\n",
        "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
        "# Show text\n",
        "remove_periods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eUKV_TMz-HP",
        "outputId": "3178c417-5693-4072-9fa1-b750e25a7da3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['   INTERROBANG BY AISHWARYA HENRIETTEPARKING AND GOING BY KARL GAUTIER',\n",
              " ' TODAY IS THE NIGHT BY JAREK PRAKASH']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create function\n",
        "def capitalizer(string: str) -> str:\n",
        "  return string.upper()\n",
        "\n",
        "# Apply function\n",
        "[capitalizer(string) for string in remove_periods]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCmnVlZX0sTV",
        "outputId": "caa1f8c5-de56-4de5-b063-639e1837e21b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['   XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXXXXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
              " ' XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import library\n",
        "import re\n",
        "# Create function\n",
        "\n",
        "def replace_letters_with_X(string: str) -> str:\n",
        "  return re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
        "\n",
        "# Apply function\n",
        "[replace_letters_with_X(string) for string in remove_periods]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai3dYw-L1sm4",
        "outputId": "b2b07f81-dc45-4770-a5ae-80c01161aa81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5|True|False|False|False|b'machine learning in python cookbook'|machine learning in python cookbook\n"
          ]
        }
      ],
      "source": [
        "# Define a string\n",
        "s = \"machine learning in python cookbook\"\n",
        "# Find the first index of the letter \"n\"\n",
        "find_n = s.find(\"n\")\n",
        "# Whether or not the string starts with \"m\"\n",
        "starts_with_m = s.startswith(\"m\")\n",
        "# Whether or not the string ends with \"python\"\n",
        "ends_with_python = s.endswith(\"python\")\n",
        "# Is the string alphanumeric\n",
        "is_alnum = s.isalnum()\n",
        "# Is it composed of only alphabetical characters (not including spaces)\n",
        "is_alpha = s.isalpha()\n",
        "# Encode as utf-8\n",
        "encode_as_utf8 = s.encode(\"utf-8\")\n",
        "# Decode the same utf-8\n",
        "decode = encode_as_utf8.decode(\"utf-8\")\n",
        "print(\n",
        "  find_n,\n",
        "  starts_with_m,\n",
        "  ends_with_python,\n",
        "  is_alnum,\n",
        "  is_alpha,\n",
        "  encode_as_utf8,\n",
        "  decode,\n",
        "  sep = \"|\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJMwQCWj4HvV",
        "outputId": "b28dad3d-64de-4a30-bd42-376ba6206aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "  find_n,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paWEo3WN4Mg5",
        "outputId": "76f6af11-d3c0-4f27-8706-9ce97d0e5ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(starts_with_m)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmUTFE534RME",
        "outputId": "3c1bd831-12e8-4c2b-ebca-a96998e6a161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(ends_with_python)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVj5tqsA4VRI",
        "outputId": "96509a15-3418-44fc-ffb0-cf14ce772709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(is_alnum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-nx8DwJ4Zzs",
        "outputId": "f9642229-a5e9-4dbf-90fe-fb2e6a2bad23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "print(is_alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN0C7ZPP4fcf",
        "outputId": "4b14987e-0e44-4af1-b96d-e07c3038f411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'machine learning in python cookbook'\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "  encode_as_utf8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuobaBNL4jFa",
        "outputId": "6b348500-19fc-4f34-9787-0c00c28d66e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "machine learning in python cookbook\n"
          ]
        }
      ],
      "source": [
        "print(decode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiPpjujd4plD"
      },
      "source": [
        "6.3 Removing Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vGyGQ5q4nau",
        "outputId": "2648d95c-50f2-4ef4-9a82-bb4925e43206"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load libraries\n",
        "import unicodedata\n",
        "import sys\n",
        "# Create text\n",
        "text_data = ['Hi!!!! I. Love. This. Song....',\n",
        "'10000% Agree!!!! #LoveIT',\n",
        "'Right?!?!']\n",
        "# Create a dictionary of punctuation characters\n",
        "punctuation = dict.fromkeys(\n",
        "(i for i in range(sys.maxunicode)\n",
        "if unicodedata.category(chr(i)).startswith('P')\n",
        "),\n",
        "None\n",
        ")\n",
        "# For each string, remove any punctuation characters\n",
        "[string.translate(punctuation) for string in text_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1O5Az8fN8M8c"
      },
      "outputs": [],
      "source": [
        "dict1 = {\"water\":\"آب\", \"why\":\"چراااااا\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo63s7Mk8c5J",
        "outputId": "dd7541cf-4377-4662-a074-1d99f743ee33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'water': 'آب', 'why': 'چراااااا'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "110Y4GWm8fwe"
      },
      "outputs": [],
      "source": [
        "s = ['why water']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lx1I7B278j1-",
        "outputId": "37ecbd5c-1f64-4d42-acd4-24b2f55fb5f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[string.translate(dict1) for string in s]'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"[string.translate(dict1) for string in s]\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLCFpG2N9-Zj",
        "outputId": "a7510e93-10ce-4137-9480-5e4ab5ec7809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['چراااااا آب']\n"
          ]
        }
      ],
      "source": [
        "dict1 = {\"water\": \"آب\", \"why\": \"چراااااا\"}\n",
        "\n",
        "s = ['why water']\n",
        "\n",
        "translated = [' '.join([dict1.get(word, word) for word in sentence.split()]) for sentence in s]\n",
        "\n",
        "print(translated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTHlyju45LXp"
      },
      "source": [
        "6.4 Tokenizing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8gTEUjtBUKb",
        "outputId": "dff9d206-5f3a-4bed-cfc7-d81db3376779"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "nltk.download('punkt_tab')\n",
        "import nltk\n",
        "# Load library\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Create text\n",
        "string = \"The science of today is the technology of tomorrow\"\n",
        "# Tokenize words\n",
        "word_tokenize(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_a_ga5PB2X9",
        "outputId": "5cc062f0-0bb5-4b5d-badc-b86dd7a7686d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load library\n",
        "from nltk.tokenize import sent_tokenize\n",
        "# Create text\n",
        "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
        "# Tokenize sentences\n",
        "sent_tokenize(string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-RoipT0BI_i"
      },
      "source": [
        "6.5 Removing Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx1mBpea8AgW",
        "outputId": "76ac9c28-51cb-49d3-b913-1b60346b0e43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['going', 'go', 'store', 'park']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "# Load library\n",
        "from nltk.corpus import stopwords\n",
        "# You will have to download the set of stop words the first time\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# Create word tokens\n",
        "tokenized_words = ['i',\n",
        "                  'am',\n",
        "                  'going',\n",
        "                  'to',\n",
        "                  'go',\n",
        "                  'to',\n",
        "                  'the',\n",
        "                  'store',\n",
        "                  'and',\n",
        "                  'park']\n",
        "# Load stop words\n",
        "stop_words = stopwords.words('english')\n",
        "# Remove stop words\n",
        "[word for word in tokenized_words if word not in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smBVLcoa_Nu7",
        "outputId": "ecc7df5f-bc6e-41c5-d803-3d24cf2048ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a', 'about', 'above', 'after', 'again']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show stop words\n",
        "stop_words[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFLop00rB7LZ"
      },
      "source": [
        "6.5 Removing Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok1sAnBBB9v_",
        "outputId": "b9a94b3e-0733-45f4-f6e1-9207bcdc8eb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load library\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "# Create word tokens\n",
        "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
        "# Create stemmer\n",
        "porter = PorterStemmer()\n",
        "# Apply stemmer\n",
        "[porter.stem(word) for word in tokenized_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qrIOsWUCDeZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOstfS2v1+JW0i+36u3zYUQ",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
